---
title: "Model Building"
output: html_notebook
---
This is to organize our models. Pull the code cleaning the data from data_preprocessing.Rmd.
This file is separate because we have visualizations and other attempts to see how to clean the data the best in that file.
This file contains our best cleaning methods.
dd
Load the data and clean it:
```{r}
rm(list=ls())
par(mfrow=c(1,1))

library(tidyverse)
library(leaps) # contanis regsubset()
library(fastDummies) # to make dummy variables
library(corrplot) # for visualization but probably not needed in final.
library(boot) # load library boot to use cv.glm
library(glmnet) # for ridge regression and lasso regression
library(caret) # another group said this was very helpful for getting pretty data that got them an MSE of 8 with LASSO, but I still get 13

raw_data <- read.csv2("./LCdata.csv", header = TRUE, row.names=NULL, sep=";")

my_data <- raw_data

# drop columns per Gwen
my_data <- select(my_data, -c("collection_recovery_fee", "installment", "issue_d", "last_pymnt_amnt", "last_pymnt_d", "loan_status", "next_pymnt_d", "out_prncp", "out_prncp_inv", "pymnt_plan", "recoveries", "term", "total_pymnt", "total_pymnt_inv", "total_rec_int", "total_rec_late_fee", "total_rec_prncp")) 

# drop columns that are not useful

my_data <- select(my_data, -c("id", "member_id", "emp_title", "url", "desc", "title", "zip_code", "addr_state", "earliest_cr_line", "last_credit_pull_d", "policy_code"))

# my_data <- subset(my_data, select = -funded_amnt) I think this is more important than initially believed, but when I checked the data I found it did not make a difference. Leaving in anyway.

# convert character to floats
my_data$int_rate <- as.double(my_data$int_rate)
my_data$dti <- as.double(my_data$dti)
my_data$dti_joint <- as.double(my_data$dti_joint)
my_data$il_util <- as.double(my_data$il_util)
my_data$all_util <- as.double(my_data$all_util)
my_data$revol_util <- as.double(my_data$revol_util)

# convert character to integer
my_data$annual_inc <- as.integer(my_data$annual_inc)
my_data$annual_inc_joint <- as.integer(my_data$annual_inc_joint)
my_data$funded_amnt_inv <- as.integer(my_data$funded_amnt_inv)

# convert character to ordered integers
# Note: should this be categorical? or group them?
my_data$emp_length <- as.integer(ordered(my_data$emp_length, levels = c("n/a", "< 1 year", "1 year", "2 years", "3 years", "4 years", "5 years", "6 years", "7 years", "8 years", "9 years", "10+ years"))) - 1 # does this need to consider na as unemployed?

# drop NA
# my_data <- my_data %>%  # note this drops 40,000 records
#  drop_na(emp_length) # this no longer does anything since n/a replaced above.
my_data <- filter(my_data, ! is.na(my_data$home_ownership))
my_data <- filter(my_data, ! is.na(my_data$annual_inc))
my_data <- filter(my_data, ! is.na(my_data$delinq_2yrs))
my_data <- filter(my_data, ! is.na(my_data$revol_bal))
my_data <- filter(my_data, ! is.na(my_data$revol_util))
my_data <- filter(my_data, ! is.na(my_data$collections_12_mths_ex_med))

# fix verification_status and verification_status_joint
my_data$verification_status <- ifelse(my_data$verification_status == "Source Verified", "Source", my_data$verification_status)
my_data$verification_status <- ifelse(my_data$verification_status == "Not Verified", "Not", my_data$verification_status)

#dummy columns
my_data <- dummy_columns(my_data, select_columns = "home_ownership", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "verification_status", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "purpose", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "application_type", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "verification_status_joint", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "initial_list_status", remove_first_dummy = TRUE, remove_selected_columns = TRUE)

# remove outliers
my_data$annual_inc <- ifelse(my_data$annual_inc >= 1000000, my_data$annual_inc / 100, my_data$annual_inc)

# Correct data errors
my_data$inq_last_12m <- ifelse(my_data$inq_last_12m < 0, my_data$inq_last_12m * -1, my_data$inq_last_12m) # eliminate negatives

# deal with joint income and joint dti
debt_payment.ind <- my_data$dti * my_data$annual_inc
debt_payment.joint <- my_data$dti_joint * my_data$annual_inc_joint
debt_payment.applicant <- ifelse(my_data$application_type_JOINT == 1, debt_payment.ind + debt_payment.joint, debt_payment.ind)

annual_inc.applicant <- ifelse(my_data$application_type_JOINT == 1, my_data$annual_inc + my_data$annual_inc_joint, my_data$annual_inc)

dti.applicant <- debt_payment.applicant / annual_inc.applicant

my_data$annual_inc <- annual_inc.applicant

my_data$dti <- ifelse(is.na(dti.applicant), my_data$dti, dti.applicant)

# drop joint data now that is not used
my_data <- select(my_data, -c("annual_inc_joint", "dti_joint"))


model_data <- my_data #make test modeling data
```






column 24 is causing problems, not sure why as it seems as though it should work.

```{r}
colSums(is.na(model_data))
```

# Create a dataset where we have credit report information
```{r}
cr_data <- filter(my_data, ! is.na(inq_last_12m))
colSums(is.na(cr_data))
# if total_bal_il is 0, make il_util 0 as well
cr_data$il_util <- ifelse(cr_data$total_bal_il == 0, 0, cr_data$il_util)
colSums(is.na(cr_data))
```

## deal with people who are not delinquent
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_delinq), y = int_rate))


```
People who are NA are lower, as we expect

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_delinq)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_delinq <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_delinq, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_delinq))
```


Make it categorical. People who have defaulted previously, and those who have not.
```{r}

cr_data$mths_since_last_delinq <- ifelse(is.na(cr_data$mths_since_last_delinq), 0, 1)

```

Do the same for other parameters. 

### mths_since_last_record
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_record), y = int_rate))

```
People who are NA are lower, as we expect

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_record)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_record <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_record, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_delinq))
```
Make it categorical. People who have defaulted previously, and those who have not. In this case, however, it may make more sense to set n/a people to a high number, since the biggest effect is seen 1/3 of the way through the graph.
```{r}
levels(factor(cr_data$mths_since_last_record))
```

Set it to be one higher than the current highest
```{r}

cr_data$mths_since_last_record <- ifelse(is.na(cr_data$mths_since_last_record), 121, cr_data$mths_since_last_record)

```

### mths_since_last_major_derog 
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_major_derog), y = int_rate))

```
People who are NA are lower, as we expect, but the median is about the same.

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_major_derog)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_major_derog <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_major_derog, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_major_derog))
```
Make it categorical because there does not seem to be a clear trend, perhaps due to lack of data.

Set it to be one higher than the current highest
```{r}

cr_data$mths_since_last_major_derog <- ifelse(is.na(cr_data$mths_since_last_major_derog), 0, 1)

```


### il_util
```{r}
filter(cr_data, is.na(il_util))
```
```{r}
ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(il_util), y = int_rate))

ggplot(data = cr_data) +
  geom_point(aes(x = il_util, y = int_rate))



```

If it is NA, set it == 0
```{r}
cr_data$il_util <- ifelse(is.na(cr_data$il_util), 0, cr_data$il_util)

```


### mths_since_rcnt_il

```{r}
ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_rcnt_il), y = int_rate))

ggplot(data = cr_data) +
  geom_point(aes(x = mths_since_rcnt_il, y = int_rate))
```
There is a good correlation here, between the parameter and int_rate, and not much between na and not na. I am not sure what to do. It would be too many observations to drop. Maybe the mean, or let caret fill it.

## make training and test set
```{r}
set.seed(1)
cr_trainData <- sample(1:nrow(cr_data), 0.7*nrow(cr_data))
cr_data.train <- cr_data[cr_trainData,]
cr_data.test <- cr_data[-cr_trainData,]

```

Make it ready for caret
```{r}
data.caret.train <- cr_data.train

data.caret.test <- cr_data.test 
```
caret ridge regression MSE = [1] 10.08767
caret LASSo MSE = [1] 10.06132
This is better than the overall dataset, which is around 13.


# Build a Linear Model

Create list of the NA columns
These are not used for now, but further data processing may make them useful.
```{r}
force_out <- c("mths_since_last_delinq", "mths_since_last_record", "mths_since_last_major_derog", "annual_inc_joint", "dti_joint", "tot_coll_amt", "tot_cur_bal", "open_acc_6m", "open_il_6m", "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util", "total_rev_hi_lim", "inq_fi", "total_cu_tl", "inq_last_12m")
```

```{r}
model_data_no_na <- select(model_data, -force_out)
colSums(is.na(model_data_no_na))
```

```{r}
set.seed(1)
trainData <- sample(1:nrow(model_data_no_na), 0.7*nrow(model_data_no_na))
data.train <- model_data_no_na[trainData,]
data.test <- model_data_no_na[-trainData,]
```


1 linear dependency found. What parameters are co-linear?
```{r}
sets_FWS <- regsubsets(int_rate ~ ., data.train, nvmax = 40, method = "forward")
summary(sets_FWS)

```


```{r}
initialize
glm1 <- glm(int_rate ~ revol_util, data = data.train)
glm2 <- glm(int_rate ~ revol_util + inq_last_6mths, data = data.train)
glm3 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card, data = data.train)
glm4 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified, data = data.train)
glm5 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc, data = data.train)
glm6 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv, data = data.train)
glm7 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation, data = data.train)
glm8 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified`, data = data.train)
glm9 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE, data = data.train)
glm10 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w, data = data.train)
glm11 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal, data = data.train)
glm12 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti, data = data.train)
glm13 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec, data = data.train)
glm14 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs, data = data.train)
glm15 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt, data = data.train)
glm16 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement, data = data.train)
glm17 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase, data = data.train)
glm18 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc, data = data.train)
glm19 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc, data = data.train)
glm20 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq, data = data.train)
glm21 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business, data = data.train)
glm22 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other, data = data.train)
glm23 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving, data = data.train)
glm24 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving + purpose_house, data = data.train)
glm25 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + pub_rec + delinq_2yrs + total_pymnt + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving + purpose_house + purpose_medical, data = data.train)
```
# Cross-validation for parameter selection

We do a 10-fold cross-validation

```{r}
set.seed(1)
cv.err.glm1 <- cv.glm(data.train, glm1, K = 10) 
cv.err.glm2 <- cv.glm(data.train, glm2, K = 10) 
cv.err.glm3 <- cv.glm(data.train, glm3, K = 10) 
cv.err.glm4 <- cv.glm(data.train, glm4, K = 10) 
cv.err.glm5 <- cv.glm(data.train, glm5, K = 10)
cv.err.glm6 <- cv.glm(data.train, glm6, K = 10) 
cv.err.glm7 <- cv.glm(data.train, glm7, K = 10) 
cv.err.glm8 <- cv.glm(data.train, glm8, K = 10)
cv.err.glm9 <- cv.glm(data.train, glm9, K = 10)
cv.err.glm10 <- cv.glm(data.train, glm10, K = 10)
cv.err.glm11 <- cv.glm(data.train, glm11, K = 10)
cv.err.glm12 <- cv.glm(data.train, glm12, K = 10)
cv.err.glm13 <- cv.glm(data.train, glm13, K = 10)
cv.err.glm14 <- cv.glm(data.train, glm14, K = 10)
cv.err.glm15 <- cv.glm(data.train, glm15, K = 10)
cv.err.glm16 <- cv.glm(data.train, glm16, K = 10)
cv.err.glm17 <- cv.glm(data.train, glm17, K = 10)
cv.err.glm18 <- cv.glm(data.train, glm18, K = 10)
cv.err.glm19 <- cv.glm(data.train, glm19, K = 10)
cv.err.glm20 <- cv.glm(data.train, glm20, K = 10)
cv.err.glm21 <- cv.glm(data.train, glm21, K = 10)
cv.err.glm22 <- cv.glm(data.train, glm22, K = 10)
cv.err.glm23 <- cv.glm(data.train, glm23, K = 10)
cv.err.glm24 <- cv.glm(data.train, glm24, K = 10)
cv.err.glm25 <- cv.glm(data.train, glm25, K = 10)
```


We plot the CV error as a function of the number of predictors
```{r}
x <- 1:25 # define the x-axes (number of predictors)
(cv.err <- c(cv.err.glm1$delta[1], cv.err.glm2$delta[1], cv.err.glm3$delta[1], cv.err.glm4$delta[1],                      cv.err.glm5$delta[1],cv.err.glm6$delta[1],cv.err.glm7$delta[1], cv.err.glm8$delta[1], cv.err.glm9$delta[1], cv.err.glm10$delta[1], cv.err.glm11$delta[1], cv.err.glm12$delta[1], cv.err.glm13$delta[1], cv.err.glm14$delta[1], cv.err.glm15$delta[1], cv.err.glm16$delta[1], cv.err.glm17$delta[1], cv.err.glm18$delta[1], cv.err.glm19$delta[1], cv.err.glm20$delta[1], cv.err.glm21$delta[1], cv.err.glm22$delta[1], cv.err.glm23$delta[1], cv.err.glm24$delta[1], cv.err.glm25$delta[1]))
plot(cv.err ~x)
lines(cv.err ~x)
```
 [1] 17.86582 16.62822 15.86358 15.33236 14.99542 14.43929 14.23070 14.05053 13.93383 13.83293 13.71637 13.60494 13.53000 13.45494 13.39076 13.32795 13.28365
[18] 13.23878 13.19996 13.18435 13.17490 13.16831 13.15635 13.14714 13.13729



Here it's hard again to see which model wins. 
To see it more clearly, we find the minimum:

```{r}
(cv.min <- which.min(cv.err))
cv.err[cv.min]

```

Which model is it? Model 25. So we need more parameters to reduce underfitting / bias.

```{r}
coef(sets_FWS,25)
summary(glm25, correlation = TRUE)
```
*Highly correlated:*
Should we remove some of them?
purpose_credit_card <-> purpose_debt_consolidation
purpose_credit_card <-> purpose_home_improvement
purpose_credit_card <-> purpose_other
purpose_debt_consolidation <-> purpose_home_improvement
purpose_debt_consolidation <-> purpose_other

Some of the other purposes are semi-highly correlated (60-70). May be worth a look.

*Note* Gwen confirmed that none of this matters because each loan would only have one purpose.


# Tune the model with the parameters we found by increasing flexibility
Note that this will not find the optimal model. Maybe int_rate ~ poly(revol_util,3) and nothing else is the best model, and this will never test that.

Note: This will take a long time to run. The idea is to run it for each parameter we choose.
```{r}
cv.error.10=rep(0,10)
for (i in 1:10){
    glm.fit = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(annual_inc,10) + poly(funded_amnt_inv,10) + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + poly(pub_rec,3) + poly(delinq_2yrs,2) + purpose_home_improvement + poly(total_pymnt,i) + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical, data = data.train)
    cv.error.10[i] = cv.glm(data.train,glm.fit,K=5)$delta[1] # Setting K=5 means 5-fold CV
}
cv.error.10
```

best poly, k = 10
revol_util: 1, but 2 is very close. After 3, goes quickly downhill
[1] 1.313748e+01 1.313807e+01 1.335145e+01 1.615801e+01 4.257502e+03 9.881411e+05 2.779848e+08 1.779192e+09 4.979685e+14 2.452146e+15

inq_last_6mths: 7, but may prefer 3 or 4 to keep it from overfitting. Or go with 1 becaus none of these are that different. Looking at the model run on test data, the p value starts being too large after 4, so we will go with 4.
[1] 13.13672 13.08867 13.07047 13.06873 13.06840 13.06889 13.06828 13.07053 13.06983 13.08498

annual_inc: 10, but may prefer 3 or 4 to keep it from overfitting. Or go with 1 becaus none of these are that different. 7 has a very similar standard error but a better p value in the fitted model.
[1] 13.06796 12.85943 12.78952 12.76273 12.76079 12.76030 12.76030 12.75972 12.75960 12.75934

from here, only a k = 5
funded_amnt_inv: 1, 2, 5, or 10. Or go with 1 becaus none of these are that different.
[1] 12.75973 12.72842 12.75470 12.70118 12.67908 13.77973 12.66829 12.65892 12.64501 12.64331

revol_bal: 3 or 1
 [1]     12.64375     12.62409     12.54643     12.54751     12.93488     27.34618    289.19445   8785.86063 533956.06421 224162.51917

dti: 1
 [1] 1.254469e+01 1.269465e+01 1.281557e+01 1.070648e+02 1.321001e+03 1.250976e+06 6.864439e+07 6.882024e+08 2.999944e+11 5.651942e+15

pub_rec: 3 Or go with 1 becaus none of these are that different on the left side.
 [1]   12.55580   12.53535   12.53258   12.54563   12.81418   12.53925   12.65237   27.34062 1119.21629  152.12983

delinq_2yrs: 4 is the lowest, but not really different from 2
 [1]   14.16914   12.50662   12.50743   12.49501   12.57875   12.61406   12.50057   18.79284   19.55115 1092.33130

total_pymnt: 8 is the minimum. But not really different from 1
 [1] 12.50376 12.50533 13.49381 12.47696 12.45606 12.45684 13.94445 12.45178 12.46844 12.47446

total_acc:

open_acc: 

acc_now_delinq:


See if a 5-fold gives us anything different
```{r}
set.seed(1)
cv.err.glm1.5 <- cv.glm(data.train, glm1, K = 5) 
cv.err.glm2.5 <- cv.glm(data.train, glm2, K = 5) 
cv.err.glm3.5 <- cv.glm(data.train, glm3, K = 5) 
cv.err.glm4.5 <- cv.glm(data.train, glm4, K = 5) 
cv.err.glm5.5 <- cv.glm(data.train, glm5, K = 5)
cv.err.glm6.5 <- cv.glm(data.train, glm6, K = 5) 
cv.err.glm7.5 <- cv.glm(data.train, glm7, K = 5) 
cv.err.glm8.5 <- cv.glm(data.train, glm8, K = 5)
cv.err.glm9.5 <- cv.glm(data.train, glm9, K = 5)
cv.err.glm10.5 <- cv.glm(data.train, glm10, K = 5)
cv.err.glm11.5 <- cv.glm(data.train, glm11, K = 5)
cv.err.glm12.5 <- cv.glm(data.train, glm12, K = 5)
cv.err.glm13.5 <- cv.glm(data.train, glm13, K = 5)
cv.err.glm14.5 <- cv.glm(data.train, glm14, K = 5)
cv.err.glm15.5 <- cv.glm(data.train, glm15, K = 5)
cv.err.glm16.5 <- cv.glm(data.train, glm16, K = 5)
cv.err.glm17.5 <- cv.glm(data.train, glm17, K = 5)
cv.err.glm18.5 <- cv.glm(data.train, glm18, K = 5)
cv.err.glm19.5 <- cv.glm(data.train, glm19, K = 5)
cv.err.glm20.5 <- cv.glm(data.train, glm20, K = 5)
cv.err.glm21.5 <- cv.glm(data.train, glm21, K = 5)
cv.err.glm22.5 <- cv.glm(data.train, glm22, K = 5)
cv.err.glm23.5 <- cv.glm(data.train, glm23, K = 5)
cv.err.glm24.5 <- cv.glm(data.train, glm24, K = 5)
cv.err.glm25.5 <- cv.glm(data.train, glm25, K = 5)
```



We plot the CV error as a function of the number of predictors
```{r}
x <- 1:25 # define the x-axes (number of predictors)
(cv.err.5 <- c(cv.err.glm1.5$delta[1], cv.err.glm2.5$delta[1], cv.err.glm3.5$delta[1], cv.err.glm4.5$delta[1],                      cv.err.glm5.5$delta[1],cv.err.glm6.5$delta[1],cv.err.glm7.5$delta[1], cv.err.glm8.5$delta[1], cv.err.glm9.5$delta[1], cv.err.glm10.5$delta[1], cv.err.glm11.5$delta[1], cv.err.glm12.5$delta[1], cv.err.glm13.5$delta[1], cv.err.glm14.5$delta[1], cv.err.glm15.5$delta[1], cv.err.glm16.5$delta[1], cv.err.glm17.5$delta[1], cv.err.glm18.5$delta[1], cv.err.glm19.5$delta[1], cv.err.glm20.5$delta[1], cv.err.glm21.5$delta[1], cv.err.glm22.5$delta[1], cv.err.glm23.5$delta[1], cv.err.glm24.5$delta.5[1], cv.err.glm25.5$delta[1]))


(cv.min.5 <- which.min(cv.err.5))
cv.err.5[cv.min.5]
```
It gives me model 24 as the best





Stepwise backward selection
```{r}
sets_BWS <- regsubsets(int_rate ~ ., data.train, nvmax = 40, method = "backward")
summary(sets_BWS)
```

```{r}
par(mfrow=c(2,3))
# FWS
plot(sets_FWS, scale="adjr2", title(main="Foreward Stepwise"))
plot(sets_FWS, scale="Cp", title(main="Foreward Stepwise"))
plot(sets_FWS, scale="bic", title(main="Foreward Stepwise"))
# BWS
plot(sets_FWS, scale="adjr2", title(main="Backward Stepwise"))
plot(sets_FWS, scale="Cp", title(main="Backward Stepwise"))
plot(sets_FWS, scale="bic", title(main="Backward Stepwise"))
par(mfrow=c(1,1))
```






# Fit the parameters to the model

```{r}
glm.fit.best = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(annual_inc,7) + poly(funded_amnt_inv,10) + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + poly(pub_rec,3) + poly(delinq_2yrs,2) + purpose_home_improvement + poly(total_pymnt,8) + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical, data = data.train)
summary(glm.fit.best)
```


# Test the fitted model

```{r}
glm.fit.best.pred <- predict(glm.fit.best, newdata = data.test)

```

```{r}
int_rate.test <- data.test$int_rate
plot(glm.fit.best.pred,int_rate.test)
abline (0,1)
(MSE_glm <- mean((data.test$int_rate-glm.fit.best.pred)^2)) 

```
This is not great. We need to increase the slope. Also why are some interest rate predictions negative?


# Ridge Regression

```{r}
predictors <- model.matrix(int_rate ~ ., data.train)[,-1] #remove the first column, which is the intercept. glmnet doesn't need that
head(predictors)
outputs <- data.train$int_rate # creating a vector to hold the response variable

```
```{r}
m_ridge.1 <- glmnet(predictors, outputs, alpha = 0)
dim(coef(m_ridge.1))
m_ridge.1
plot(m_ridge.1, label=TRUE)
```
## cross-validation to select the best lambda
```{r}
(cv_ridge <- cv.glmnet(predictors, outputs, alpha = 0) )

```
Measure: Mean-Squared Error 

    Lambda Index Measure      SE Nonzero
min 0.1167   100   13.15 0.04104      40
1se 0.3565    88   13.19 0.04016      40

```{r}
plot(cv_ridge)
(best_lambda <- cv_ridge$lambda.min)
coef(m_ridge.1, s=best_lambda)

```

## Training the model on ridge regression
```{r}
predictors.Train <- model.matrix(int_rate ~ ., data.train)[,-1] # prepare format for glmnet
outputs.Train <- data.train$int_rate  # prepare format for glmnet
m_ridge.Train <- glmnet(predictors.Train, outputs.Train, alpha = 0) # do the fit

```


## Test the ridge regression

We can use the predict() function to make predictions with ridge regression
```{r}
predictors.Test <- model.matrix(int_rate ~ ., data.test)[,-1] # prepare format for glmnet
ridge.pred <- predict(m_ridge.Train, newx = predictors.Test, s = best_lambda) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_ridge <- mean((data.test$int_rate-ridge.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model.



```{r}
plot(ridge.pred,int_rate.test)
abline (0,1)
```

This has the same issue that it needs to increase the slope. It overpredicts on the bottom end and underpredicts on the top end.

# LASSO

```{r}
m_LASSO <- glmnet(predictors, outputs, alpha = 1)
dim(coef(m_LASSO))
```


```{r}
m_LASSO # we see that some of the coefficients are set to 0
plot(m_LASSO, label = TRUE)
(cv_LASSO <- cv.glmnet(predictors, outputs, alpha = 1))
plot(cv_LASSO)
(best_lambda_LASSO <- cv_LASSO$lambda.min) #best lambda is very small, but lambda 1se is considerably larger
coef(m_LASSO, s = best_lambda_LASSO) # for small lambda almost all coefficients are included
coef(m_LASSO, s = cv_LASSO$lambda.1se) # We can try again with lambda.1se
coef(m_ridge.1, s = best_lambda) # we can compare with ridge regression

```

## Training the LASSO model
```{r}
predictors.LASSO.Train <- model.matrix(int_rate ~ ., data.train)[,-1] # prepare format for glmnet
outputs.LASSO.Train <- data.train$int_rate  # prepare format for glmnet
m_ridge.LASSO.Train <- glmnet(predictors.LASSO.Train, outputs.LASSO.Train, alpha = 1) # do the fit

```

## Test the LASSO regression

We can use the predict() function to make predictions with LASSO regression
```{r}
predictors.LASSO.Test <- model.matrix(int_rate ~ ., data.test)[,-1] # prepare format for glmnet
LASSO.pred <- predict(m_ridge.LASSO.Train, newx = predictors.LASSO.Test, s = best_lambda_LASSO) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_LASSO <- mean((data.test$int_rate-LASSO.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model, or the ridge regression.



```{r}
plot(LASSO.pred,int_rate.test)
abline (0,1)
```

The exact same problem appears, where it over predicts the interest rate on the low end and under predicts it on the high end.

# Poly models
We need to fix the slope issue.
```{r}
cv.error.10=rep(0,10)
for (i in 1:10){
    glm.fit.poly1 = glm(int_rate ~ poly(revol_util,i), data = data.train)
    cv.error.10[i] = cv.glm(data.train,glm.fit,K=5)$delta[1] # Setting K=5 means 5-fold CV
}
cv.error.10
```
[1] 25.16243 25.15573 25.19109 25.21601 27.53920 25.20314 25.23525 25.22842 25.97635 25.20173
poly(revol_util,2) is slightly better than 1, but only just.

# Standardize data
Maybe standardizing the data will improve the fit. (it does not)
```{r}
model_standard <- model_data_no_na %>% mutate_each_(list(~scale(.) %>% as.vector),
                                  vars = c("revol_util", "inq_last_6mths", "annual_inc", "funded_amnt_inv", "revol_bal", "dti", "pub_rec", "delinq_2yrs", "total_pymnt", "total_acc", "open_acc", "acc_now_delinq", "loan_amnt")) #loan_amnt is added but not used
```

## Build test and training set
```{r}
trainData.standard <- sample(1:nrow(model_standard), 0.7*nrow(model_standard))
data.standard.train <- model_standard[trainData,]
data.standard.test <- model_standard[-trainData,]
```

## Fit the parameters to the model

```{r}
glm.standard.fit.best = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(annual_inc,7) + poly(funded_amnt_inv,10) + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + poly(pub_rec,3) + poly(delinq_2yrs,2) + purpose_home_improvement + poly(total_pymnt,8) + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical, data = data.standard.train)
summary(glm.standard.fit.best)
```


## Test the fitted model

```{r}
glm.fit.standard.best.pred <- predict(glm.standard.fit.best, newdata = data.standard.test)

```

```{r}
int_rate.test <- data.standard.test$int_rate
plot(glm.fit.standard.best.pred,int_rate.test)
abline (0,1)
(MSE_standard <- mean((data.standard.test$int_rate-glm.fit.standard.best.pred)^2)) 

```
This did not improve the situation. MSE went down to 12.39, which is exactly the same as in the glm model.


# Caret library

```{r}
set.seed(1)
trainData.caret <- sample(1:nrow(model_data), 0.7*nrow(model_data))
data.caret.train <- model_data[trainData.caret,]
data.caret.test <- model_data[-trainData.caret,]
```


## Data Pre-processing

```{r}
caret_data <- model_data
```

### Impute Data
```{r}

```


### Remove zero- and near-zero-variance predictors
```{r}
nzv <- nearZeroVar(caret_data)
caret_data <- caret_data[,-nzv]
```

### Remove correlated predictors

This code does not work
Error in if (x[i, j] > cutoff) { : missing value where TRUE/FALSE needed
```{r}
data_cor <- cor(caret_data)
data_highlyCor <- findCorrelation(data_cor, cutoff = .75)
caret_data <- caret_data[,-data_highlyCor]
```

### Remove linear dependencies

```{r}
comboInfo <- findLinearCombos(caret_data)
caret_data[,comboInfo$remove]
```

### Impute

## knn - Not working


```{r}
preprocPar.knn <- preProcess(data.caret.train, method = c("scale", "center", "knnImpute", "nzv"))

train_transformed.knn <- predict(preprocPar, data.caret.train)
test_transformed.knn <- predict(preprocPar, data.caret.test)
```


## bagged trees

```{r}
preprocPar.bagged <- preProcess(data.caret.train, method = c("scale", "center", "bagImpute", "nzv"))

train_transformed.bagged <- predict(preprocPar.bagged, data.caret.train)
test_transformed.bagged <- predict(preprocPar.bagged, data.caret.test)
```

## Ridge Regression

```{r}
caret.predictors <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] #remove the first column, which is the intercept. glmnet doesn't need that
head(caret.predictors)
outputs <- data.caret.train$int_rate # creating a vector to hold the response variable
# here, I keep int_rate untransformed, but maybe revisit

```

```{r}
m_ridge.caret <- glmnet(caret.predictors, outputs, alpha = 0)
dim(coef(m_ridge.caret))
m_ridge.caret
plot(m_ridge.caret, label=TRUE)
```
### cross-validation to select the best lambda
```{r}
(cv_ridge.caret <- cv.glmnet(caret.predictors, outputs, alpha = 0) )

```
Measure: Mean-Squared Error 

    Lambda Index Measure      SE Nonzero
min 0.1167   100   13.15 0.04104      40
1se 0.3565    88   13.19 0.04016      40

```{r}
plot(cv_ridge.caret)
(best_lambda.caret <- cv_ridge.caret$lambda.min)
coef(m_ridge.caret, s=best_lambda.caret)

```

### Training the model on ridge regression
```{r}
predictors.Train <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] # prepare format for glmnet
outputs.Train <- data.caret.train$int_rate  # prepare format for glmnet
m_ridge.Train <- glmnet(predictors.Train, outputs.Train, alpha = 0) # do the fit

```


### Test the ridge regression

We can use the predict() function to make predictions with ridge regression
```{r}
predictors.Test <- model.matrix(int_rate ~ ., test_transformed.bagged)[,-1] # prepare format for glmnet
ridge.pred <- predict(m_ridge.Train, newx = predictors.Test, s = best_lambda.caret) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_ridge <- mean((data.caret.test$int_rate-ridge.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model.



```{r}
plot(ridge.pred,data.caret.test$int_rate)
abline (0,1)
```

This has the same issue that it needs to increase the slope. It overpredicts on the bottom end and underpredicts on the top end.

## LASSO

```{r}
m_LASSO <- glmnet(caret.predictors, outputs, alpha = 1)
dim(coef(m_LASSO))
```


```{r}
m_LASSO # we see that some of the coefficients are set to 0
plot(m_LASSO, label = TRUE)
(cv_LASSO <- cv.glmnet(caret.predictors, outputs, alpha = 1))
plot(cv_LASSO)
(best_lambda_LASSO <- cv_LASSO$lambda.min) #best lambda is very small, but lambda 1se is considerably larger
coef(m_LASSO, s = best_lambda_LASSO) # for small lambda almost all coefficients are included
coef(m_LASSO, s = cv_LASSO$lambda.1se) # We can try again with lambda.1se
coef(m_ridge.caret, s = best_lambda.caret) # we can compare with ridge regression

```

### Training the LASSO model
```{r}
predictors.LASSO.Train <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] # prepare format for glmnet
outputs.LASSO.Train <- data.caret.train$int_rate  # prepare format for glmnet
m_ridge.LASSO.Train <- glmnet(predictors.LASSO.Train, outputs.LASSO.Train, alpha = 1) # do the fit

```

### Test the LASSO regression

We can use the predict() function to make predictions with LASSO regression
```{r}
predictors.LASSO.Test <- model.matrix(int_rate ~ ., test_transformed.bagged)[,-1] # prepare format for glmnet
LASSO.pred <- predict(m_ridge.LASSO.Train, newx = predictors.LASSO.Test, s = best_lambda_LASSO) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_LASSO <- mean((data.caret.test$int_rate-LASSO.pred)^2)) 
          
```
This is not a significantly different MSE from the ridge regression.



```{r}
plot(LASSO.pred,data.caret.test$int_rate)
abline (0,1)
```

The exact same problem appears, where it over predicts the interest rate on the low end and under predicts it on the high end.