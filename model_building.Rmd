---
title: "Model Building"
output: html_notebook
---
This is to organize our models. Pull the code cleaning the data from data_preprocessing.Rmd.
This file is separate because we have visualizations and other attempts to see how to clean the data the best in that file.
This file contains our best cleaning methods.
dd
Load the data and clean it:

The caret GBM model is the currently the best with a test MSE of 6.162478.
The caret MARS model is second best with a test MSE of 6.56588

```{r}
rm(list=ls())
par(mfrow=c(1,1))

library(tidyverse)
library(leaps) # contanis regsubset()
library(fastDummies) # to make dummy variables
library(corrplot) # for visualization but probably not needed in final.
library(boot) # load library boot to use cv.glm
library(glmnet) # for ridge regression and lasso regression
library(caret) # very helpful for training a model
library(earth)
library(gbm)

#library(doParallel) # this is to run on multiple cores of your cpu.
#cl <- makePSOCKcluster(7) # the number here is the number of cores to use
#registerDoParallel(cl)

raw_data <- read.csv2("./LCdata.csv", header = TRUE, row.names=NULL, sep=";")

my_data <- raw_data

# drop columns per Gwen
my_data <- select(my_data, -c("collection_recovery_fee", "installment", "issue_d", "last_pymnt_amnt", "last_pymnt_d", "loan_status", "next_pymnt_d", "out_prncp", "out_prncp_inv", "pymnt_plan", "recoveries", "term", "total_pymnt", "total_pymnt_inv", "total_rec_int", "total_rec_late_fee", "total_rec_prncp")) 

# drop columns that are not useful

my_data <- select(my_data, -c("id", "member_id", "emp_title", "url", "desc", "title", "zip_code", "addr_state", "earliest_cr_line", "last_credit_pull_d", "policy_code"))

# my_data <- subset(my_data, select = -funded_amnt) I think this is more important than initially believed, but when I checked the data I found it did not make a difference. Leaving in anyway.

# convert character to floats
my_data$int_rate <- as.double(my_data$int_rate)
my_data$dti <- as.double(my_data$dti)
my_data$dti_joint <- as.double(my_data$dti_joint)
my_data$il_util <- as.double(my_data$il_util)
my_data$all_util <- as.double(my_data$all_util)
my_data$revol_util <- as.double(my_data$revol_util)

# convert character to integer
my_data$annual_inc <- as.integer(my_data$annual_inc)
my_data$annual_inc_joint <- as.integer(my_data$annual_inc_joint)
my_data$funded_amnt_inv <- as.integer(my_data$funded_amnt_inv)

# convert character to ordered integers
# Note: should this be categorical? or group them?
my_data$emp_length <- as.integer(ordered(my_data$emp_length, levels = c("n/a", "< 1 year", "1 year", "2 years", "3 years", "4 years", "5 years", "6 years", "7 years", "8 years", "9 years", "10+ years"))) - 1 # does this need to consider na as unemployed?

# drop NA
my_data <- filter(my_data, ! is.na(my_data$home_ownership))
my_data <- filter(my_data, ! is.na(my_data$annual_inc))
my_data <- filter(my_data, ! is.na(my_data$delinq_2yrs))
my_data <- filter(my_data, ! is.na(my_data$revol_bal))
my_data <- filter(my_data, ! is.na(my_data$revol_util))
my_data <- filter(my_data, ! is.na(my_data$collections_12_mths_ex_med))

# fix verification_status and verification_status_joint
my_data$verification_status <- ifelse(my_data$verification_status == "Source Verified", "Source", my_data$verification_status)
my_data$verification_status <- ifelse(my_data$verification_status == "Not Verified", "Not", my_data$verification_status)
my_data$verification_status_joint <- ifelse(my_data$verification_status == "Source Verified", "Source", my_data$verification_status)
my_data$verification_status_joint <- ifelse(my_data$verification_status == "Not Verified", "Not", my_data$verification_status)

#dummy columns
my_data <- dummy_columns(my_data, select_columns = "home_ownership", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "verification_status", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "purpose", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "application_type", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "verification_status_joint", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
my_data <- dummy_columns(my_data, select_columns = "initial_list_status", remove_first_dummy = TRUE, remove_selected_columns = TRUE)

# Correct data errors
my_data$inq_last_12m <- ifelse(my_data$inq_last_12m < 0, my_data$inq_last_12m * -1, my_data$inq_last_12m) # eliminate negatives

# deal with joint income and joint dti
debt_payment.ind <- my_data$dti * my_data$annual_inc
debt_payment.joint <- my_data$dti_joint * my_data$annual_inc_joint
debt_payment.applicant <- ifelse(my_data$application_type_JOINT == 1, debt_payment.ind + debt_payment.joint, debt_payment.ind)

annual_inc.applicant <- ifelse(my_data$application_type_JOINT == 1, my_data$annual_inc + my_data$annual_inc_joint, my_data$annual_inc)

dti.applicant <- debt_payment.applicant / annual_inc.applicant

my_data$annual_inc <- annual_inc.applicant

my_data$dti <- ifelse(is.na(dti.applicant), my_data$dti, dti.applicant)

# drop joint data now that is not used
my_data <- select(my_data, -c("annual_inc_joint", "dti_joint"))

# remove outliers
my_data$annual_inc <- ifelse(my_data$annual_inc >= 1000000, my_data$annual_inc / 100, my_data$annual_inc)

# remove outliers with a k value > 3 or < -3


model_data <- my_data #make test modeling data
```


Try to transform the y variable to make more normal, less rightward skewed
```{r}
ggplot(my_data, aes(x = int_rate)) +
  geom_histogram(binwidth = 1)

ggplot(my_data, aes(x = log(int_rate))) +
  geom_histogram(binwidth = .1)

ggplot(my_data, aes(x = sqrt(int_rate))) +
  geom_histogram(binwidth = .1)

```




column 24 is causing problems, not sure why as it seems as though it should work.

```{r}
colSums(is.na(model_data))
```

# Create a dataset where we have credit report information
```{r}
cr_data <- filter(my_data, ! is.na(inq_last_12m))
colSums(is.na(cr_data))
# if total_bal_il is 0, make il_util 0 as well
cr_data$il_util <- ifelse(cr_data$total_bal_il == 0, 0, cr_data$il_util)
colSums(is.na(cr_data))
```

## deal with people who are not delinquent
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_delinq), y = int_rate))


```
People who are NA are lower, as we expect

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_delinq)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_delinq <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_delinq, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_delinq))
```


Make it categorical. People who have defaulted previously, and those who have not.
```{r}

cr_data$mths_since_last_delinq <- ifelse(is.na(cr_data$mths_since_last_delinq), 0, 1)

```

Do the same for other parameters. 

### mths_since_last_record
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_record), y = int_rate))

```
People who are NA are lower, as we expect

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_record)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_record <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_record, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_delinq))
```
Make it categorical. People who have defaulted previously, and those who have not. In this case, however, it may make more sense to set n/a people to a high number, since the biggest effect is seen 1/3 of the way through the graph.
```{r}
levels(factor(cr_data$mths_since_last_record))
```

Set it to be one higher than the current highest
```{r}

cr_data$mths_since_last_record <- ifelse(is.na(cr_data$mths_since_last_record), 121, cr_data$mths_since_last_record)

```

### mths_since_last_major_derog 
```{r}

ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_last_major_derog), y = int_rate))

```
People who are NA are lower, as we expect, but the median is about the same.

This is to see if we can find a similar time period. But no, not really
```{r}
df <- cr_data
f <- as.character(df$mths_since_last_major_derog)
f <- ifelse(is.na(f), "n/a", f)
head(f)
df$mths_since_last_major_derog <- f

ggplot(data = df) +
  geom_boxplot(mapping = aes(x = mths_since_last_major_derog, y = int_rate))

ggplot(data = df) +
  geom_bar(aes(x = mths_since_last_major_derog))
```
Make it categorical because there does not seem to be a clear trend, perhaps due to lack of data.

Set it to be one higher than the current highest
```{r}

cr_data$mths_since_last_major_derog <- ifelse(is.na(cr_data$mths_since_last_major_derog), 0, 1)

```


### il_util
```{r}
filter(cr_data, is.na(il_util))
```
```{r}
ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(il_util), y = int_rate))

ggplot(data = cr_data) +
  geom_point(aes(x = il_util, y = int_rate))



```

If it is NA, set it == 0
```{r}
cr_data$il_util <- ifelse(is.na(cr_data$il_util), 0, cr_data$il_util)

```


### mths_since_rcnt_il

```{r}
ggplot(data = cr_data) +
  geom_boxplot(mapping = aes(x = is.na(mths_since_rcnt_il), y = int_rate))

ggplot(data = cr_data) +
  geom_point(aes(x = mths_since_rcnt_il, y = int_rate))
```
There is a good correlation here, between the parameter and int_rate, and not much between na and not na. I am not sure what to do. It would be too many observations to drop. Maybe the mean, or let caret fill it.

## make training and test set
```{r}
set.seed(1)
cr_trainData <- sample(1:nrow(cr_data), 0.7*nrow(cr_data))
cr_data.train <- cr_data[cr_trainData,]
cr_data.test <- cr_data[-cr_trainData,]

```

Make it ready for caret
```{r}
data.caret.train <- cr_data.train

data.caret.test <- cr_data.test 
```
caret ridge regression MSE = [1] 10.08767
caret LASSo MSE = [1] 10.06132
This is better than the overall dataset, which is around 13.


# Build a Linear Model

Create list of the NA columns
These are not used for now, but further data processing may make them useful.
```{r}
force_out <- c("mths_since_last_delinq", "mths_since_last_record", "mths_since_last_major_derog", "tot_coll_amt", "tot_cur_bal", "open_acc_6m", "open_il_6m", "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util", "total_rev_hi_lim", "inq_fi", "total_cu_tl", "inq_last_12m")
```

```{r}
model_data_no_na <- select(model_data, -force_out)
colSums(is.na(model_data_no_na))
```

```{r}
set.seed(1)
trainData <- sample(1:nrow(model_data_no_na), 0.7*nrow(model_data_no_na))
data.train <- model_data_no_na[trainData,]
data.test <- model_data_no_na[-trainData,]
```


1 linear dependency found. What parameters are co-linear?
```{r}
sets_FWS <- regsubsets(int_rate ~ ., data.train, nvmax = 39, method = "forward")
summary(sets_FWS)

```
```{r}
lm1 <- lm(int_rate ~ annual_inc, data = data.train)
lm2 <- lm(int_rate ~ log(annual_inc), data = data.train) # much better than lm1
lm3 <- lm(int_rate ~ sqrt(annual_inc), data = data.train)
lm4 <- lm(int_rate ~ poly(annual_inc,10), data = data.train)
lm5 <- lm(int_rate ~ poly(log(annual_inc), 7), data = data.train) # best
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)

lm1 <- lm(int_rate ~ revol_util, data = data.train) # best
lm2 <- lm(int_rate ~ log(revol_util), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(revol_util), data = data.train)
lm4 <- lm(int_rate ~ poly(revol_util,10), data = data.train)
lm5 <- lm(int_rate ~ poly(log(revol_util), 10), data = data.train) # can't take log
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)


lm1 <- lm(int_rate ~ inq_last_6mths, data = data.train) # best
lm2 <- lm(int_rate ~ log(inq_last_6mths), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(inq_last_6mths), data = data.train)
lm4 <- lm(int_rate ~ poly(inq_last_6mths,4), data = data.train)
lm5 <- lm(int_rate ~ poly(log(inq_last_6mths), 10), data = data.train) # can't take log
summary(lm1) # not far behind
summary(lm2)
summary(lm3) # also good
summary(lm4) # best
summary(lm5)

lm1 <- lm(int_rate ~ revol_bal, data = data.train) # best
#lm2 <- lm(int_rate ~ log(revol_bal), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(revol_bal), data = data.train)
lm4 <- lm(int_rate ~ poly(revol_bal,3), data = data.train)
#lm5 <- lm(int_rate ~ poly(log(revol_bal), 3), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) 
summary(lm4) # best
#summary(lm5)

lm1 <- lm(int_rate ~ dti, data = data.train) # best
#lm2 <- lm(int_rate ~ log(dti), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(dti), data = data.train)
lm4 <- lm(int_rate ~ poly(dti,3), data = data.train)
#lm5 <- lm(int_rate ~ poly(log(dti), 3), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) 
summary(lm4) # best
#summary(lm5)

lm1 <- lm(int_rate ~ pub_rec, data = data.train) # best
#lm2 <- lm(int_rate ~ log(pub_rec), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(pub_rec), data = data.train)
lm4 <- lm(int_rate ~ poly(pub_rec,3), data = data.train)
#lm5 <- lm(int_rate ~ poly(log(pub_rec), 3), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) # best 
summary(lm4)
#summary(lm5)

lm1 <- lm(int_rate ~ delinq_2yrs, data = data.train) # best
#lm2 <- lm(int_rate ~ log(pub_rec), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(delinq_2yrs), data = data.train)
lm4 <- lm(int_rate ~ poly(delinq_2yrs,2), data = data.train)
#lm5 <- lm(int_rate ~ poly(log(pub_rec), 3), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) # best 
summary(lm4)
#summary(lm5)

lm1 <- lm(int_rate ~ total_acc, data = data.train) # best
#lm2 <- lm(int_rate ~ log(pub_rec), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(total_acc), data = data.train)
lm4 <- lm(int_rate ~ poly(total_acc,2), data = data.train)
lm5 <- lm(int_rate ~ poly(total_acc, 4), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) # good
summary(lm4) # better
summary(lm5) # best


lm1 <- lm(int_rate ~ open_acc, data = data.train) # best
#lm2 <- lm(int_rate ~ log(pub_rec), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(open_acc), data = data.train)
lm4 <- lm(int_rate ~ poly(open_acc,2), data = data.train)
lm5 <- lm(int_rate ~ poly(open_acc, 4), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) # good
summary(lm4) # best
summary(lm5)

lm1 <- lm(int_rate ~ acc_now_delinq, data = data.train) # best
#lm2 <- lm(int_rate ~ log(pub_rec), data = data.train) # can't take log
lm3 <- lm(int_rate ~ sqrt(acc_now_delinq), data = data.train)
lm4 <- lm(int_rate ~ poly(acc_now_delinq,2), data = data.train)
lm5 <- lm(int_rate ~ poly(acc_now_delinq, 4), data = data.train) # can't take log
summary(lm1) 
#summary(lm2)
summary(lm3) # good
summary(lm4) # best
summary(lm5)

```


```{r}
initialize
glm1 <- glm(int_rate ~ revol_util, data = data.train)
glm2 <- glm(int_rate ~ revol_util + inq_last_6mths, data = data.train)
glm3 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card, data = data.train)
glm4 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified, data = data.train)
glm5 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc, data = data.train)
glm6 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv, data = data.train)
glm7 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation, data = data.train)
glm8 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source, data = data.train)
glm9 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE, data = data.train)
glm10 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w, data = data.train)
glm11 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal, data = data.train)
glm12 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti, data = data.train)
glm13 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs, data = data.train)
glm14 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec, data = data.train)
glm15 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement, data = data.train)
glm16 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase, data = data.train)
glm17 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc, data = data.train)
glm18 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc, data = data.train)
glm19 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq, data = data.train)
glm20 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business, data = data.train)
glm21 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other, data = data.train)
glm22 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving, data = data.train)
glm23 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving + purpose_house, data = data.train)
glm24 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving + purpose_house + purpose_medical, data = data.train)
glm25 <- glm(int_rate ~ revol_util + inq_last_6mths + purpose_credit_card + verification_status_Verified + annual_inc + funded_amnt_inv + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + initial_list_status_w + revol_bal + dti + delinq_2yrs + pub_rec + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_other + purpose_moving + purpose_house + purpose_medical + purpose_vacation, data = data.train)
```
# Cross-validation for parameter selection

We do a 10-fold cross-validation

```{r}
set.seed(1)
cv.err.glm1 <- cv.glm(data.train, glm1, K = 10) 
cv.err.glm2 <- cv.glm(data.train, glm2, K = 10) 
cv.err.glm3 <- cv.glm(data.train, glm3, K = 10) 
cv.err.glm4 <- cv.glm(data.train, glm4, K = 10) 
cv.err.glm5 <- cv.glm(data.train, glm5, K = 10)
cv.err.glm6 <- cv.glm(data.train, glm6, K = 10) 
cv.err.glm7 <- cv.glm(data.train, glm7, K = 10) 
cv.err.glm8 <- cv.glm(data.train, glm8, K = 10)
cv.err.glm9 <- cv.glm(data.train, glm9, K = 10)
cv.err.glm10 <- cv.glm(data.train, glm10, K = 10)
cv.err.glm11 <- cv.glm(data.train, glm11, K = 10)
cv.err.glm12 <- cv.glm(data.train, glm12, K = 10)
cv.err.glm13 <- cv.glm(data.train, glm13, K = 10)
cv.err.glm14 <- cv.glm(data.train, glm14, K = 10)
cv.err.glm15 <- cv.glm(data.train, glm15, K = 10)
cv.err.glm16 <- cv.glm(data.train, glm16, K = 10)
cv.err.glm17 <- cv.glm(data.train, glm17, K = 10)
cv.err.glm18 <- cv.glm(data.train, glm18, K = 10)
cv.err.glm19 <- cv.glm(data.train, glm19, K = 10)
cv.err.glm20 <- cv.glm(data.train, glm20, K = 10)
cv.err.glm21 <- cv.glm(data.train, glm21, K = 10)
cv.err.glm22 <- cv.glm(data.train, glm22, K = 10)
cv.err.glm23 <- cv.glm(data.train, glm23, K = 10)
cv.err.glm24 <- cv.glm(data.train, glm24, K = 10)
cv.err.glm25 <- cv.glm(data.train, glm25, K = 10)
```


We plot the CV error as a function of the number of predictors
```{r}
x <- 1:25 # define the x-axes (number of predictors)
(cv.err <- c(cv.err.glm1$delta[1], cv.err.glm2$delta[1], cv.err.glm3$delta[1], cv.err.glm4$delta[1],                      cv.err.glm5$delta[1],cv.err.glm6$delta[1],cv.err.glm7$delta[1], cv.err.glm8$delta[1], cv.err.glm9$delta[1], cv.err.glm10$delta[1], cv.err.glm11$delta[1], cv.err.glm12$delta[1], cv.err.glm13$delta[1], cv.err.glm14$delta[1], cv.err.glm15$delta[1], cv.err.glm16$delta[1], cv.err.glm17$delta[1], cv.err.glm18$delta[1], cv.err.glm19$delta[1], cv.err.glm20$delta[1], cv.err.glm21$delta[1], cv.err.glm22$delta[1], cv.err.glm23$delta[1], cv.err.glm24$delta[1], cv.err.glm25$delta[1]))
plot(cv.err ~x)
lines(cv.err ~x)
```
 [1] 17.86582 16.62822 15.86358 15.33236 14.99542 14.43929 14.23070 14.05053 13.93383 13.83293 13.71637 13.60494 13.53000 13.45494 13.39076 13.32795 13.28365
[18] 13.23878 13.19996 13.18435 13.17490 13.16831 13.15635 13.14714 13.13729



Here it's hard again to see which model wins. 
To see it more clearly, we find the minimum:

```{r}
(cv.min <- which.min(cv.err))
cv.err[cv.min]

```

Which model is it? Model 25. So we need more parameters to reduce underfitting / bias.

```{r}
coef(sets_FWS,25)
summary(glm25, correlation = TRUE)
```
*Highly correlated:*
Should we remove some of them?
purpose_credit_card <-> purpose_debt_consolidation
purpose_credit_card <-> purpose_home_improvement
purpose_credit_card <-> purpose_other
purpose_debt_consolidation <-> purpose_home_improvement
purpose_debt_consolidation <-> purpose_other

Some of the other purposes are semi-highly correlated (60-70). May be worth a look.

*Note* Gwen confirmed that none of this matters because each loan would only have one purpose.


# Tune the model with the parameters we found by increasing flexibility
Note that this will not find the optimal model. Maybe int_rate ~ poly(revol_util,3) and nothing else is the best model, and this will never test that.

Note: This will take a long time to run. The idea is to run it for each parameter we choose.
```{r}
cv.error.10=rep(0,10)
for (i in 1:10){
    glm.fit = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(log(annual_inc), 7) + poly(funded_amnt_inv,4) + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + sqrt(pub_rec) + sqrt(delinq_2yrs) + purpose_home_improvement + purpose_major_purchase + poly(total_acc,4) + poly(open_acc,2) + poly(acc_now_delinq,2) + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical + purpose_vacation, data = data.train)
    cv.error.10[i] = cv.glm(data.train,glm.fit,K=5)$delta[1] # Setting K=5 means 5-fold CV
}
cv.error.10

```

best poly, k = 10
revol_util: 1, but 2 is very close. After 3, goes quickly downhill
[1] 1.313748e+01 1.313807e+01 1.335145e+01 1.615801e+01 4.257502e+03 9.881411e+05 2.779848e+08 1.779192e+09 4.979685e+14 2.452146e+15

inq_last_6mths: 7, but may prefer 3 or 4 to keep it from overfitting. Or go with 1 becaus none of these are that different. Looking at the model run on test data, the p value starts being too large after 4, so we will go with 4.
[1] 13.13672 13.08867 13.07047 13.06873 13.06840 13.06889 13.06828 13.07053 13.06983 13.08498

annual_inc: 10, but may prefer 3 or 4 to keep it from overfitting. Or go with 1 becaus none of these are that different. 7 has a very similar standard error but a better p value in the fitted model.
[1] 13.06796 12.85943 12.78952 12.76273 12.76079 12.76030 12.76030 12.75972 12.75960 12.75934

from here, only a k = 5
funded_amnt_inv: 1, 2, 5, or 10. Or go with 1 becaus none of these are that different.
[1] 12.75973 12.72842 12.75470 12.70118 12.67908 13.77973 12.66829 12.65892 12.64501 12.64331

revol_bal: 3 or 1
 [1]     12.64375     12.62409     12.54643     12.54751     12.93488     27.34618    289.19445   8785.86063 533956.06421 224162.51917

dti: 1
 [1] 1.254469e+01 1.269465e+01 1.281557e+01 1.070648e+02 1.321001e+03 1.250976e+06 6.864439e+07 6.882024e+08 2.999944e+11 5.651942e+15

pub_rec: 3 Or go with 1 becaus none of these are that different on the left side.
 [1]   12.55580   12.53535   12.53258   12.54563   12.81418   12.53925   12.65237   27.34062 1119.21629  152.12983

delinq_2yrs: 4 is the lowest, but not really different from 2
 [1]   14.16914   12.50662   12.50743   12.49501   12.57875   12.61406   12.50057   18.79284   19.55115 1092.33130

total_acc:

open_acc: 

acc_now_delinq:


See if a 5-fold gives us anything different
```{r}
set.seed(1)
cv.err.glm1.5 <- cv.glm(data.train, glm1, K = 5) 
cv.err.glm2.5 <- cv.glm(data.train, glm2, K = 5) 
cv.err.glm3.5 <- cv.glm(data.train, glm3, K = 5) 
cv.err.glm4.5 <- cv.glm(data.train, glm4, K = 5) 
cv.err.glm5.5 <- cv.glm(data.train, glm5, K = 5)
cv.err.glm6.5 <- cv.glm(data.train, glm6, K = 5) 
cv.err.glm7.5 <- cv.glm(data.train, glm7, K = 5) 
cv.err.glm8.5 <- cv.glm(data.train, glm8, K = 5)
cv.err.glm9.5 <- cv.glm(data.train, glm9, K = 5)
cv.err.glm10.5 <- cv.glm(data.train, glm10, K = 5)
cv.err.glm11.5 <- cv.glm(data.train, glm11, K = 5)
cv.err.glm12.5 <- cv.glm(data.train, glm12, K = 5)
cv.err.glm13.5 <- cv.glm(data.train, glm13, K = 5)
cv.err.glm14.5 <- cv.glm(data.train, glm14, K = 5)
cv.err.glm15.5 <- cv.glm(data.train, glm15, K = 5)
cv.err.glm16.5 <- cv.glm(data.train, glm16, K = 5)
cv.err.glm17.5 <- cv.glm(data.train, glm17, K = 5)
cv.err.glm18.5 <- cv.glm(data.train, glm18, K = 5)
cv.err.glm19.5 <- cv.glm(data.train, glm19, K = 5)
cv.err.glm20.5 <- cv.glm(data.train, glm20, K = 5)
cv.err.glm21.5 <- cv.glm(data.train, glm21, K = 5)
cv.err.glm22.5 <- cv.glm(data.train, glm22, K = 5)
cv.err.glm23.5 <- cv.glm(data.train, glm23, K = 5)
cv.err.glm24.5 <- cv.glm(data.train, glm24, K = 5)
cv.err.glm25.5 <- cv.glm(data.train, glm25, K = 5)
```



We plot the CV error as a function of the number of predictors
```{r}
x <- 1:25 # define the x-axes (number of predictors)
(cv.err.5 <- c(cv.err.glm1.5$delta[1], cv.err.glm2.5$delta[1], cv.err.glm3.5$delta[1], cv.err.glm4.5$delta[1],                      cv.err.glm5.5$delta[1],cv.err.glm6.5$delta[1],cv.err.glm7.5$delta[1], cv.err.glm8.5$delta[1], cv.err.glm9.5$delta[1], cv.err.glm10.5$delta[1], cv.err.glm11.5$delta[1], cv.err.glm12.5$delta[1], cv.err.glm13.5$delta[1], cv.err.glm14.5$delta[1], cv.err.glm15.5$delta[1], cv.err.glm16.5$delta[1], cv.err.glm17.5$delta[1], cv.err.glm18.5$delta[1], cv.err.glm19.5$delta[1], cv.err.glm20.5$delta[1], cv.err.glm21.5$delta[1], cv.err.glm22.5$delta[1], cv.err.glm23.5$delta[1], cv.err.glm24.5$delta.5[1], cv.err.glm25.5$delta[1]))


(cv.min.5 <- which.min(cv.err.5))
cv.err.5[cv.min.5]
```
It gives me model 24 as the best





Stepwise backward selection
```{r}
sets_BWS <- regsubsets(int_rate ~ ., data.train, nvmax = 40, method = "backward")
summary(sets_BWS)
```

```{r}
par(mfrow=c(2,3))
# FWS
plot(sets_FWS, scale="adjr2", title(main="Foreward Stepwise"))
plot(sets_FWS, scale="Cp", title(main="Foreward Stepwise"))
plot(sets_FWS, scale="bic", title(main="Foreward Stepwise"))
# BWS
plot(sets_FWS, scale="adjr2", title(main="Backward Stepwise"))
plot(sets_FWS, scale="Cp", title(main="Backward Stepwise"))
plot(sets_FWS, scale="bic", title(main="Backward Stepwise"))
par(mfrow=c(1,1))
```






# Fit the parameters to the model

```{r}
glm.fit.best = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(log(annual_inc), 7) + poly(funded_amnt_inv,4) + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + sqrt(pub_rec) + sqrt(delinq_2yrs) + purpose_home_improvement + purpose_major_purchase + poly(total_acc,4) + poly(open_acc,2) + poly(acc_now_delinq,2) + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical + purpose_vacation, data = data.train)
summary(glm.fit.best)
```
square root model
```{r}
glm.fit.best = glm(int_rate ~ revol_util + sqrt(inq_last_6mths) + purpose_credit_card + verification_status_Verified + log(annual_inc) + sqrt(funded_amnt_inv) + purpose_debt_consolidation + verification_status_Source + home_ownership_MORTGAGE + sqrt(revol_bal) + dti + initial_list_status_w + sqrt(pub_rec) + sqrt(delinq_2yrs) + sqrt(total_acc) + sqrt(open_acc) + sqrt(acc_now_delinq) + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical + purpose_vacation, data = data.train) # home improvement and major purchase removed
summary(glm.fit.best)
```


# Test the fitted model

```{r}
glm.fit.best.pred <- predict(glm.fit.best, newdata = data.test)

```

```{r}
int_rate.test <- data.test$int_rate
plot(glm.fit.best.pred,int_rate.test)
abline (0,1)
(MSE_glm <- mean((data.test$int_rate-glm.fit.best.pred)^2)) 

```
first poly model[1] 12.5818
square root model: [1] 12.69812
This is not great. We need to increase the slope. Also why are some interest rate predictions negative?


# Ridge Regression

```{r}
predictors <- model.matrix(int_rate ~ ., data.train)[,-1] #remove the first column, which is the intercept. glmnet doesn't need that
head(predictors)
outputs <- data.train$int_rate # creating a vector to hold the response variable

```
```{r}
m_ridge.1 <- glmnet(predictors, outputs, alpha = 0)
dim(coef(m_ridge.1))
m_ridge.1
plot(m_ridge.1, label=TRUE)
```
## cross-validation to select the best lambda
```{r}
(cv_ridge <- cv.glmnet(predictors, outputs, alpha = 0) )

```
Measure: Mean-Squared Error 

    Lambda Index Measure      SE Nonzero
min 0.1167   100   13.15 0.04104      40
1se 0.3565    88   13.19 0.04016      40

```{r}
plot(cv_ridge)
(best_lambda <- cv_ridge$lambda.min)
coef(m_ridge.1, s=best_lambda)

```

## Training the model on ridge regression
```{r}
predictors.Train <- model.matrix(int_rate ~ ., data.train)[,-1] # prepare format for glmnet
outputs.Train <- data.train$int_rate  # prepare format for glmnet
m_ridge.Train <- glmnet(predictors.Train, outputs.Train, alpha = 0) # do the fit

```


## Test the ridge regression

We can use the predict() function to make predictions with ridge regression
```{r}
predictors.Test <- model.matrix(int_rate ~ ., data.test)[,-1] # prepare format for glmnet
ridge.pred <- predict(m_ridge.Train, newx = predictors.Test, s = best_lambda) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_ridge <- mean((data.test$int_rate-ridge.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model.



```{r}
plot(ridge.pred,int_rate.test)
abline (0,1)
```

This has the same issue that it needs to increase the slope. It overpredicts on the bottom end and underpredicts on the top end.

# LASSO

```{r}
m_LASSO <- glmnet(predictors, outputs, alpha = 1)
dim(coef(m_LASSO))
```


```{r}
m_LASSO # we see that some of the coefficients are set to 0
plot(m_LASSO, label = TRUE)
(cv_LASSO <- cv.glmnet(predictors, outputs, alpha = 1))
plot(cv_LASSO)
(best_lambda_LASSO <- cv_LASSO$lambda.min) #best lambda is very small, but lambda 1se is considerably larger
coef(m_LASSO, s = best_lambda_LASSO) # for small lambda almost all coefficients are included
coef(m_LASSO, s = cv_LASSO$lambda.1se) # We can try again with lambda.1se
coef(m_ridge.1, s = best_lambda) # we can compare with ridge regression

```

## Training the LASSO model
```{r}
predictors.LASSO.Train <- model.matrix(int_rate ~ ., data.train)[,-1] # prepare format for glmnet
outputs.LASSO.Train <- data.train$int_rate  # prepare format for glmnet
m_ridge.LASSO.Train <- glmnet(predictors.LASSO.Train, outputs.LASSO.Train, alpha = 1) # do the fit

```

## Test the LASSO regression

We can use the predict() function to make predictions with LASSO regression
```{r}
predictors.LASSO.Test <- model.matrix(int_rate ~ ., data.test)[,-1] # prepare format for glmnet
LASSO.pred <- predict(m_ridge.LASSO.Train, newx = predictors.LASSO.Test, s = best_lambda_LASSO) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_LASSO <- mean((data.test$int_rate-LASSO.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model, or the ridge regression.



```{r}
plot(LASSO.pred,int_rate.test)
abline (0,1)
```

The exact same problem appears, where it over predicts the interest rate on the low end and under predicts it on the high end.

# Poly models
We need to fix the slope issue.
```{r}
cv.error.10=rep(0,10)
for (i in 1:10){
    glm.fit.poly1 = glm(int_rate ~ poly(revol_util,i), data = data.train)
    cv.error.10[i] = cv.glm(data.train,glm.fit,K=5)$delta[1] # Setting K=5 means 5-fold CV
}
cv.error.10
```
[1] 25.16243 25.15573 25.19109 25.21601 27.53920 25.20314 25.23525 25.22842 25.97635 25.20173
poly(revol_util,2) is slightly better than 1, but only just.

# Standardize data
Maybe standardizing the data will improve the fit. (it does not)
```{r}
model_standard <- model_data_no_na %>% mutate_each_(list(~scale(.) %>% as.vector),
                                  vars = c("revol_util", "inq_last_6mths", "annual_inc", "funded_amnt_inv", "revol_bal", "dti", "pub_rec", "delinq_2yrs", "total_acc", "open_acc", "acc_now_delinq", "loan_amnt")) #loan_amnt is added but not used
```

## Build test and training set
```{r}
trainData.standard <- sample(1:nrow(model_standard), 0.7*nrow(model_standard))
data.standard.train <- model_standard[trainData,]
data.standard.test <- model_standard[-trainData,]
```

## Fit the parameters to the model

```{r}
glm.standard.fit.best = glm(int_rate ~ revol_util + poly(inq_last_6mths,4) + purpose_credit_card + verification_status_Verified + poly(annual_inc,7) + poly(funded_amnt_inv,10) + purpose_debt_consolidation + `verification_status_Source Verified` + home_ownership_MORTGAGE + poly(revol_bal,3) + dti + initial_list_status_w + poly(pub_rec,3) + poly(delinq_2yrs,2) + purpose_home_improvement + purpose_major_purchase + total_acc + open_acc + acc_now_delinq + purpose_small_business + purpose_moving + purpose_other + purpose_house + purpose_medical, data = data.standard.train)
summary(glm.standard.fit.best)
```


## Test the fitted model

```{r}
glm.fit.standard.best.pred <- predict(glm.standard.fit.best, newdata = data.standard.test)

```

```{r}
int_rate.test <- data.standard.test$int_rate
plot(glm.fit.standard.best.pred,int_rate.test)
abline (0,1)
(MSE_standard <- mean((data.standard.test$int_rate-glm.fit.standard.best.pred)^2)) 

```
This did not improve the situation. MSE went down to 12.39, which is exactly the same as in the glm model.


# Caret library

Run this chunk if running other Caret regressions below

```{r}
set.seed(1)
trainData.caret <- sample(1:nrow(model_data), 0.7*nrow(model_data))
data.caret.train <- model_data[trainData.caret,]
data.caret.test <- model_data[-trainData.caret,]
```

for the no_na dataset. This produces a significantly worse result than above.
```{r}
set.seed(1)
trainData.caret <- sample(1:nrow(model_data), 0.7*nrow(model_data_no_na))
data.caret.train <- model_data_no_na[trainData.caret,]
data.caret.test <- model_data_no_na[-trainData.caret,]
```

## Data Pre-processing


### Impute Data
```{r}
rm(model_data)
```


### Remove zero- and near-zero-variance predictors
```{r}
nzv <- nearZeroVar(caret_data)
caret_data <- caret_data[,-nzv]
```

### Remove correlated predictors

This code does not work
Error in if (x[i, j] > cutoff) { : missing value where TRUE/FALSE needed
```{r}
data_cor <- cor(caret_data)
data_highlyCor <- findCorrelation(data_cor, cutoff = .75)
caret_data <- caret_data[,-data_highlyCor]
```

### Remove linear dependencies

```{r}
comboInfo <- findLinearCombos(caret_data)
caret_data[,comboInfo$remove]
```

### Impute

## knn - Not working


```{r}
preprocPar.knn <- preProcess(data.caret.train, method = c("scale", "center", "knnImpute", "nzv"))

train_transformed.knn <- predict(preprocPar, data.caret.train)
test_transformed.knn <- predict(preprocPar, data.caret.test)
```


## bagged trees

```{r}
preprocPar.bagged <- preProcess(data.caret.train, method = c("scale", "center", "bagImpute", "nzv"))

train_transformed.bagged <- predict(preprocPar.bagged, data.caret.train)
test_transformed.bagged <- predict(preprocPar.bagged, data.caret.test)
```

## Ridge Regression

```{r}
caret.predictors <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] #remove the first column, which is the intercept. glmnet doesn't need that
head(caret.predictors)
outputs <- data.caret.train$int_rate # creating a vector to hold the response variable
# here, I keep int_rate untransformed, but maybe revisit

```

```{r}
m_ridge.caret <- glmnet(caret.predictors, outputs, alpha = 0)
dim(coef(m_ridge.caret))
m_ridge.caret
plot(m_ridge.caret, label=TRUE)
```
### cross-validation to select the best lambda
```{r}
(cv_ridge.caret <- cv.glmnet(caret.predictors, outputs, alpha = 0) )

```
Measure: Mean-Squared Error 

    Lambda Index Measure      SE Nonzero
min 0.1167   100   13.15 0.04104      40
1se 0.3565    88   13.19 0.04016      40

```{r}
plot(cv_ridge.caret)
(best_lambda.caret <- cv_ridge.caret$lambda.min)
coef(m_ridge.caret, s=best_lambda.caret)

```

### Training the model on ridge regression
```{r}
predictors.Train <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] # prepare format for glmnet
outputs.Train <- data.caret.train$int_rate  # prepare format for glmnet
m_ridge.Train <- glmnet(predictors.Train, outputs.Train, alpha = 0) # do the fit

```


### Test the ridge regression

We can use the predict() function to make predictions with ridge regression
```{r}
predictors.Test <- model.matrix(int_rate ~ ., test_transformed.bagged)[,-1] # prepare format for glmnet
ridge.pred <- predict(m_ridge.Train, newx = predictors.Test, s = best_lambda.caret) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_ridge <- mean((data.caret.test$int_rate-ridge.pred)^2)) 
          
```
This is not a significantly different MSE from the glm.25 or glm.fit.best model.



```{r}
plot(ridge.pred,data.caret.test$int_rate)
abline (0,1)
```

This has the same issue that it needs to increase the slope. It overpredicts on the bottom end and underpredicts on the top end.

## LASSO

```{r}
m_LASSO <- glmnet(caret.predictors, outputs, alpha = 1)
dim(coef(m_LASSO))
```


```{r}
m_LASSO # we see that some of the coefficients are set to 0
plot(m_LASSO, label = TRUE)
(cv_LASSO <- cv.glmnet(caret.predictors, outputs, alpha = 1))
plot(cv_LASSO)
(best_lambda_LASSO <- cv_LASSO$lambda.min) #best lambda is very small, but lambda 1se is considerably larger
coef(m_LASSO, s = best_lambda_LASSO) # for small lambda almost all coefficients are included
coef(m_LASSO, s = cv_LASSO$lambda.1se) # We can try again with lambda.1se
coef(m_ridge.caret, s = best_lambda.caret) # we can compare with ridge regression

```

### Training the LASSO model
```{r}
predictors.LASSO.Train <- model.matrix(int_rate ~ ., train_transformed.bagged)[,-1] # prepare format for glmnet
outputs.LASSO.Train <- data.caret.train$int_rate  # prepare format for glmnet
m_ridge.LASSO.Train <- glmnet(predictors.LASSO.Train, outputs.LASSO.Train, alpha = 1) # do the fit

```

### Test the LASSO regression

We can use the predict() function to make predictions with LASSO regression
```{r}
predictors.LASSO.Test <- model.matrix(int_rate ~ ., test_transformed.bagged)[,-1] # prepare format for glmnet
LASSO.pred <- predict(m_ridge.LASSO.Train, newx = predictors.LASSO.Test, s = best_lambda_LASSO) # s specifies the lambda to use
# Compute MSE just for fun - maybe to compare later with other models
(MSE_LASSO <- mean((data.caret.test$int_rate-LASSO.pred)^2)) 
          
```
This is not a significantly different MSE from the ridge regression.



```{r}
plot(LASSO.pred,data.caret.test$int_rate)
abline (0,1)
```

The exact same problem appears, where it over predicts the interest rate on the low end and under predicts it on the high end.

## Another attempt using caret to pick the best

Run this code to prepare trainData and preProcess_model

```{r}
preProcess_model <- preProcess(data.caret.train, method=c("scale", "center", "bagImpute", "nzv"))
trainData <- predict(preProcess_model, newdata = data.caret.train)

# Append the Y variable
trainData$int_rate <- data.caret.train$int_rate 
#trainData$int_rate <- log(data.caret.train$int_rate)
#trainData$int_rate <- sqrt(data.caret.train$int_rate)



```






## Mars model
```{r}
# Set the seed for reproducibility
set.seed(1)

# Train the model using MARS and predict on the training data itself.
model_mars = train(int_rate ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)

plot(model_mars, main="Model Accuracies with MARS")

varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")

```

What parameters does the model have
```{r}
model_mars$bestTune
```
model 3
nprune = 41
degree = 1

Try to build the model without using train()
```{r}
earth_manual <- earth(int_rate ~ ., data = trainData, degree = 1, nprune = 41)
summary(earth_manual)

```


Test Data
```{r}
testData <- predict(preProcess_model, newdata = data.caret.test)
predicted_mars <- predict(model_mars, testData)
predicted_mars_manual <- predict(earth_manual, testData)
```

```{r}
(MSE_mars <- mean((data.caret.test$int_rate-predicted_mars)^2)) 
(MSE_mars_manual <- mean((data.caret.test$int_rate-exp(predicted_mars_manual))^2)) # must square predicted mars manual if using sqrt for int_rate, or exp() if log

```
Hooray! [1] 8.240425
[1] 6.56588 this time, which seems like a big difference. Why did the seed not give the same answer?
mars_manual gives the same result!
mars_manual gives [1] 8.339944 when using sqrt of int_rate
mars_manual gives [1] 8.652331 when using log of int_rate

```{r}
plot(exp(predicted_mars_manual),data.caret.test$int_rate)
abline (0,1)


```

```{r}
ggplot(aes(x = predicted_mars, y = data.caret.test$int_rate)) +
  geom_point(alpha = 0.01) +
  geom_abline(0,1)
```


## LASSO model

```{r}
# Set the seed for reproducibility
set.seed(1)

# Train the model using randomForest and predict on the training data itself.
model_lasso = train(int_rate ~ ., data=trainData, method='lasso')
fitted_lasso <- predict(model_lasso)

plot(model_lasso, main="Model Accuracies with LASSO")

```

```{r}
model_lasso$bestTune
```
3   fraction = 0.9

Test Data
```{r}
testData_lasso <- predict(preProcess_model, newdata = data.caret.test)
predicted_lasso <- predict(model_lasso, testData_lasso)

```

```{r}
(MSE_lasso <- mean((data.caret.test$int_rate-predicted_lasso)^2)) 
          
```
[1] 11.68413
[1] 9.36435


```{r}
plot(predicted_lasso,data.caret.test$int_rate)
abline (0,1)
```

This doesn't look bad, but it's not as good as MARS.


## Gradient Boosting

```{r}
#gbmGrid <-  expand.grid(interaction.depth = c(9, 12),
#                        n.trees = 1500,
#                        shrinkage = 0.1,
#                        n.minobsinnode = 10)

#gbmGrid <-  expand.grid(interaction.depth = c(3, 6, 9, 12), # This code ran for days and didn't finish.
#                        n.trees = (1:30)*50,
#                        shrinkage = 0.1,
#                        n.minobsinnode = c(10, 20))

model_gbm <- train(int_rate ~ .,
                   data = trainData,
                   method = "gbm",
                   trControl = trainControl(method = "repeatedcv", 
                                             number = 5, 
                                             repeats = 1, 
                                             verboseIter = FALSE),
                   tuneGrid = gbmGrid,
                   verbose = 0)
model_gbm

```
The above took about two hours to run (without gbmGrid) and produced the following: 

Stochastic Gradient Boosting 

558660 samples
    41 predictor

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 3 times) 
Summary of sample sizes: 446927, 446929, 446929, 446927, 446928, 446928, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  RMSE      Rsquared   MAE     
  1                   50      3.346992  0.4583690  2.654495
  1                  100      3.123952  0.5196202  2.471047
  1                  150      3.000790  0.5527215  2.372149
  2                   50      3.022432  0.5530334  2.384909
  2                  100      2.770724  0.6159621  2.184578
  2                  150      2.640738  0.6461271  2.081313
  3                   50      2.838252  0.6033087  2.237145
  3                  100      2.599667  0.6571138  2.048011
  3                  150      2.481392  0.6851175  1.954902

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode' was held constant at a value of 10
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.


started at 12:55pm using gridTune. Still going at 06:20 four days later

```{r}
model_gbm$bestTune
```
model 9
 
n.trees 150

interaction.depth 3

shrinkage 0.1

n.minobsinnode 10


Try to build the model without using train()
```{r}
gbm_manual <- gbm(int_rate ~ ., data = trainData, n.trees = 150, interaction.depth = 3, shrinkage = 0.1, n.minobsinnode = 10)
summary(gbm_manual)

```

Using variables with rel.inf > 0
```{r}
gbm_manual.2 <- gbm(int_rate ~ open_acc_6m + all_util + inq_last_12m + revol_util + il_util + total_rev_hi_lim + open_rv_24m + funded_amnt_inv + purpose_credit_card + loan_amnt + mths_since_last_delinq + total_acc + open_rv_12m + inq_last_6mths + annual_inc + mths_since_last_major_derog + delinq_2yrs + verification_status_Verified + funded_amnt + dti + tot_cur_bal + open_acc + verification_status_Source + inq_fi + mths_since_rcnt_il + open_il_24m + pub_rec + open_il_6m, data = trainData, n.trees = 150, interaction.depth = 3, shrinkage = 0.1, n.minobsinnode = 10)
summary(gbm_manual.2)

```
Test Data
```{r}
testData <- predict(preProcess_model, newdata = data.caret.test)
predicted_gbm <- predict(model_gbm, testData)
predicted_gbm_manual <- predict(gbm_manual, testData)
predicted_gbm_manual.2 <- predict(gbm_manual.2, testData)
```

```{r}
(MSE_gbm <- mean((data.caret.test$int_rate-predicted_gbm)^2)) 
(MSE_gbm_manual <- mean((data.caret.test$int_rate-predicted_gbm_manual)^2)) 
(MSE_gbm_manual.2 <- mean((data.caret.test$int_rate-predicted_gbm_manual.2)^2)) 
```
[1] 6.162478
[1] 6.180476 the manual model is not an exact replica of the train() model, which is odd. When using the model_data_no_na, the MSE is 12.14618.
[1] 6.18611 cutting varaibles that the model says are not relevant does not significantly change the result


```{r}
plot(predicted_gbm,data.caret.test$int_rate)
abline (0,1)
```

